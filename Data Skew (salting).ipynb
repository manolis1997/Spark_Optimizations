{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd18e12-26d0-4e15-86fc-a424c032f8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6682fe9-a25b-40b4-91ca-e01f7380b08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "# Employee Data (emp)\n",
    "emp_data = [\n",
    "    (1, \"Alice\", 101, 60000),\n",
    "    (2, \"Bob\", 102, 70000),\n",
    "    (3, \"Charlie\", 103, 50000),\n",
    "    (4, \"David\", 101, 45000),\n",
    "    (5, \"Eva\", 104, 75000),\n",
    "    (6, \"Frank\", 102, 72000),\n",
    "    (7, \"Grace\", 103, 48000),\n",
    "    (8, \"Hank\", 105, 80000),\n",
    "    (9, \"Ivy\", 106, 67000),\n",
    "    (10, \"Jack\", 101, 51000),\n",
    "    (11, \"Karen\", 102, 62000),\n",
    "    (12, \"Leo\", 104, 59000),\n",
    "    (13, \"Mona\", 105, 85000),\n",
    "    (14, \"Nate\", 106, 64000),\n",
    "    (15, \"Olivia\", 101, 56000)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "# Department Data (dept)\n",
    "dept_data = [\n",
    "    (101, \"HR\"),\n",
    "    (102, \"Engineering\"),\n",
    "    (103, \"Marketing\"),\n",
    "    (104, \"Finance\"),\n",
    "    (105, \"Sales\"),\n",
    "    (106, \"IT\")\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"dept_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d582afb-0847-4941-a184-047be6e41e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = emp.join(dept, on=emp.dept_id==dept.dept_id, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7667cdb-6c1f-438c-9a1f-14516ca13d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da9b622-8219-4d9e-84e3-515eb6f57b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790a5044-a45a-4ea6-bc25-7a73016346a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "final_df.withColumn('partition_id', spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb92e18-4129-4bf3-8e3d-f1fea071cd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Skew (salting)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
