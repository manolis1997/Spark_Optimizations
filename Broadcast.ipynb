{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d2f9c3-26d5-4722-8d08-b75f271e7a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"LargerDataFrames\").getOrCreate()\n",
    "\n",
    "# Employee Data (emp)\n",
    "emp_data = [\n",
    "    (1, \"Alice\", 101, 60000),\n",
    "    (2, \"Bob\", 102, 70000),\n",
    "    (3, \"Charlie\", 103, 50000),\n",
    "    (4, \"David\", 101, 45000),\n",
    "    (5, \"Eva\", 104, 75000),\n",
    "    (6, \"Frank\", 102, 72000),\n",
    "    (7, \"Grace\", 103, 48000),\n",
    "    (8, \"Hank\", 105, 80000),\n",
    "    (9, \"Ivy\", 106, 67000),\n",
    "    (10, \"Jack\", 101, 51000),\n",
    "    (11, \"Karen\", 102, 62000),\n",
    "    (12, \"Leo\", 104, 59000),\n",
    "    (13, \"Mona\", 105, 85000),\n",
    "    (14, \"Nate\", 106, 64000),\n",
    "    (15, \"Olivia\", 101, 56000)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "# Department Data (dept)\n",
    "dept_data = [\n",
    "    (101, \"HR\"),\n",
    "    (102, \"Engineering\"),\n",
    "    (103, \"Marketing\"),\n",
    "    (104, \"Finance\"),\n",
    "    (105, \"Sales\"),\n",
    "    (106, \"IT\")\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"dept_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6958fb-26a5-4704-9fba-7327c8e50491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_df = emp.join(dept, emp.dept_id == dept.dept_id).explain()\n",
    "\n",
    "# = Physical Plan ==\n",
    "# AdaptiveSparkPlan isFinalPlan=false\n",
    "# +- SortMergeJoin [dept_id#216], [dept_id#222], Inner\n",
    "#    :- Sort [dept_id#216 ASC NULLS FIRST], false, 0\n",
    "#    :  +- Exchange hashpartitioning(dept_id#216, 200), ENSURE_REQUIREMENTS, [plan_id=391]\n",
    "#    :     +- Filter isnotnull(dept_id#216)\n",
    "#    :        +- Scan ExistingRDD[emp_id#214,emp_name#215,dept_id#216,salary#217]\n",
    "#    +- Sort [dept_id#222 ASC NULLS FIRST], false, 0\n",
    "#       +- Exchange hashpartitioning(dept_id#222, 200), ENSURE_REQUIREMENTS, [plan_id=392]\n",
    "#          +- Filter isnotnull(dept_id#222)\n",
    "#             +- Scan ExistingRDD[dept_id#222,dept_name#223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d181e8f-d88e-496f-a785-bf3327fe481e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "join_df_broadcast = emp.join(broadcast(dept), on=\"dept_id\", how=\"inner\").explain()\n",
    "\n",
    "# == Physical Plan ==\n",
    "# AdaptiveSparkPlan isFinalPlan=false\n",
    "# +- Project [dept_id#216, emp_id#214, emp_name#215, salary#217, dept_name#223]\n",
    "#    +- BroadcastHashJoin [dept_id#216], [dept_id#222], Inner, BuildRight, false, true\n",
    "#       :- Filter isnotnull(dept_id#216)\n",
    "#       :  +- Scan ExistingRDD[emp_id#214,emp_name#215,dept_id#216,salary#217]\n",
    "#       +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=474]\n",
    "#          +- Filter isnotnull(dept_id#222)\n",
    "#             +- Scan ExistingRDD[dept_id#222,dept_name#223]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Broadcast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
