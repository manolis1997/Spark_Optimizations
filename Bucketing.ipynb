{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d0d989-ab6e-4df0-8e86-0172713b22e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TABLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb90c681-0e32-4ad0-afc2-d6580123be61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Large DataFrame Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample function to generate random rows\n",
    "def generate_row(i):\n",
    "    return (\n",
    "        i,\n",
    "        f\"user_{random.randint(1, 100000)}\",\n",
    "        round(random.uniform(100.0, 1000.0), 2),\n",
    "        random.choice([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        datetime(2020, 1, 1) + timedelta(days=random.randint(0, 365 * 3))\n",
    "    )\n",
    "\n",
    "# Generate 1 million rows of data\n",
    "data = [generate_row(i) for i in range(5_000_000)]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"id\", \"username\", \"purchase_amount\", \"category\", \"signup_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbca3b36-11b8-4d13-88b2-1e0f5d350eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode(\"overwrite\").save('dbfs:/user/hive/warehouse/default/parquet_table_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7b64b2-06f9-44b8-ae96-a221dd623284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TABLE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e82dae-fe87-4917-b6ce-25f6c068c66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Large DataFrame Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample function to generate random rows\n",
    "def generate_row(i):\n",
    "    return (\n",
    "        i,\n",
    "        f\"user_{random.randint(1, 100000)}\",\n",
    "        round(random.uniform(100.0, 1000.0), 2),\n",
    "        random.choice([\"A\", \"B\", \"C\", \"D\"]),\n",
    "        datetime(2020, 1, 1) + timedelta(days=random.randint(0, 365 * 3))\n",
    "    )\n",
    "\n",
    "# Generate 1 million rows of data\n",
    "data = [generate_row(i) for i in range(3_000_000)]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"id\", \"username\", \"purchase_amount\", \"category\", \"signup_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df2 = spark.createDataFrame(data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99843f32-f857-45f9-9259-dbd9c5db0799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.write.format('parquet').mode(\"overwrite\").save('dbfs:/user/hive/warehouse/default/parquet_table_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1012d0-285c-4a06-b283-2b9d6e0b0713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark\\\n",
    "    .read.parquet('dbfs:/user/hive/warehouse/default/parquet_table_1')\\\n",
    "    .write.saveAsTable('hive_metastore.default.parquet_table_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c23d45f-cffb-4b0a-9169-d18a6c7ceeda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark\\\n",
    "    .read.parquet('dbfs:/user/hive/warehouse/default/parquet_table_2')\\\n",
    "    .write.saveAsTable('hive_metastore.default.parquet_table_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331fda61-4c25-4a5b-92bf-1d65043dc56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/user/hive/warehouse/default/parquet_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8374ba04-99af-4194-a4a5-ff1af9d9dbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/user/hive/warehouse/default/parquet_table_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b6ea19-37b3-4b0b-85a2-f9421bccc75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('parquet_table_1')\n",
    "df2 = spark.read.table('parquet_table_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e78e4a-3414-472c-9747-930aeb46aa39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.join(df2, df.id == df2.id, 'inner').explain()\n",
    "\n",
    "# == Physical Plan ==\n",
    "# AdaptiveSparkPlan isFinalPlan=false\n",
    "# +- SortMergeJoin [id#21L], [id#36L], Inner\n",
    "#    :- Sort [id#21L ASC NULLS FIRST], false, 0\n",
    "#    :  +- Exchange hashpartitioning(id#21L, 200), ENSURE_REQUIREMENTS, [plan_id=59]\n",
    "#    :     +- Project [id#21L, username#22, purchase_amount#23, category#24, signup_date#25]\n",
    "#    :        +- Filter (if (isnotnull(_databricks_internal_edge_computed_column_skip_row#351)) (_databricks_internal_edge_computed_column_skip_row#351 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(id#21L))\n",
    "#    :           +- FileScan parquet dbw_lakehouse_dev.default.parquet_table_1[id#21L,username#22,purchase_amount#23,category#24,signup_date#25,_databricks_internal_edge_computed_column_skip_row#351] Batched: true, DataFilters: [isnotnull(id#21L)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[abfss://unity-catalog-storage@dbstorage24ijpia5cltgk.dfs.core.win..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,username:string,purchase_amount:double,category:string,signup_date:timestamp,_da...\n",
    "#    +- Sort [id#36L ASC NULLS FIRST], false, 0\n",
    "#       +- Exchange hashpartitioning(id#36L, 200), ENSURE_REQUIREMENTS, [plan_id=60]\n",
    "#          +- Project [id#36L, username#37, purchase_amount#38, category#39, signup_date#40]\n",
    "#             +- Filter (if (isnotnull(_databricks_internal_edge_computed_column_skip_row#352)) (_databricks_internal_edge_computed_column_skip_row#352 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(id#36L))\n",
    "#                +- FileScan parquet dbw_lakehouse_dev.default.parquet_table_2[id#36L,username#37,purchase_amount#38,category#39,signup_date#40,_databricks_internal_edge_computed_column_skip_row#352] Batched: true, DataFilters: [isnotnull(id#36L)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[abfss://unity-catalog-storage@dbstorage24ijpia5cltgk.dfs.core.win..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,username:string,purchase_amount:double,category:string,signup_date:timestamp,_da...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d016aa-a183-4ace-82b1-f57b47e6dfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bab584f-2bfa-4f71-97a5-263ac0bb7380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(6, \"id\") \\\n",
    "    .saveAsTable(\"hive_metastore.default.bucket_parquet_table_1\", path=\"dbfs:/user/hive/warehouse/default/bucket_parquet_table_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c8309f-5215-479f-8bb5-4cb8e22da416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(6, \"id\") \\\n",
    "    .saveAsTable(\"hive_metastore.default.bucket_parquet_table_2\", path=\"dbfs:/user/hive/warehouse/default/bucket_parquet_table_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9c61c3-ab21-481b-9f5d-ebbef289a1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bucket_parquet_table_1 = spark.table(\"hive_metastore.default.bucket_parquet_table_1\")\n",
    "bucket_parquet_table_2 = spark.table(\"hive_metastore.default.bucket_parquet_table_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f7c866-e76e-4823-a5ef-d7a1806c0268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bucket_parquet_table_1.join(bucket_parquet_table_2, bucket_parquet_table_1.id == bucket_parquet_table_2.id, 'inner').explain()\n",
    "\n",
    "# == Physical Plan ==\n",
    "# AdaptiveSparkPlan isFinalPlan=false\n",
    "# +- SortMergeJoin [id#3238L], [id#3248L], Inner\n",
    "#    :- Sort [id#3238L ASC NULLS FIRST], false, 0\n",
    "#    :  +- Filter isnotnull(id#3238L)\n",
    "#    :     +- FileScan parquet hive_metastore.default.bucket_parquet_table_1[id#3238L,username#3239,purchase_amount#3240,category#3241,signup_date#3242] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#3238L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/default/bucket_parquet_table_1], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,username:string,purchase_amount:double,category:string,signup_date:timestamp>, SelectedBucketsCount: 6 out of 6\n",
    "#    +- Sort [id#3248L ASC NULLS FIRST], false, 0\n",
    "#       +- Filter isnotnull(id#3248L)\n",
    "#          +- FileScan parquet hive_metastore.default.bucket_parquet_table_2[id#3248L,username#3249,purchase_amount#3250,category#3251,signup_date#3252] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#3248L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/user/hive/warehouse/default/bucket_parquet_table_2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,username:string,purchase_amount:double,category:string,signup_date:timestamp>, SelectedBucketsCount: 6 out of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b54c01e-d3b0-4386-a444-ea9023adfa75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/user/hive/warehouse/default/bucket_parquet_table_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c550e6-23eb-4d5e-b323-3c5ae93435ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/user/hive/warehouse/default/bucket_parquet_table_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a411f4-0a9c-465d-b32c-d45a6210edb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\", \"false\")\n",
    "display(spark.read.parquet('dbfs:/user/hive/warehouse/default/bucket_parquet_table_1/part-00000-tid-7955972004986354342-491a57ba-3200-4226-b645-45fd6d36fa8b-65-3_00002.c000.snappy.parquet').select('id').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c65709-67fb-483a-9629-f9ce5d302195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\", \"false\")\n",
    "display(spark.read.parquet('dbfs:/user/hive/warehouse/default/bucket_parquet_table_2/part-00000-tid-7249542870130387305-7fd44ef9-9ca8-4e05-b078-4b978aa306fb-69-3_00002.c000.snappy.parquet').select('id').distinct())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4726765405085318,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bucketing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
