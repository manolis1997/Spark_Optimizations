{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad393a93-e87f-4bc7-a224-346a1bb82d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, when\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "source_data = [\n",
    "    (101, 'Alice', 'Boston', 'MA'),\n",
    "    (102, 'Bob', 'Aigaleo', 'IL'),\n",
    "    (104, 'Charlie', 'Austin', 'TX')\n",
    "]\n",
    "source_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "])\n",
    "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
    "\n",
    "target_data = [\n",
    "    (101, 'Alice', 'New York', 'NY', '2022-01-01', '2023-03-15', 'N'),\n",
    "    (101, 'Alice', 'Boston', 'MA', '2023-03-16', None, 'Y'),\n",
    "    (102, 'Bob', 'Chicago', 'IL', '2022-01-01', None, 'Y'),\n",
    "    (103, 'Charlie', 'Austin', 'TX', '2022-01-01', None, 'Y'),\n",
    "]\n",
    "target_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('start_date', StringType(), True),\n",
    "    StructField('end_date', StringType(), True),\n",
    "    StructField('is_current', StringType(), True),\n",
    "])\n",
    "target_df = spark.createDataFrame(target_data, schema=target_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9b0021-1aea-4725-9979-0d46288167d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(source_df)\n",
    "display(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a57de8-f418-42c3-a03c-52454cdfe23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Left join between Source and Target table on customer_id and is_current=Y (active records)\n",
    "join_df = source_df.\\\n",
    "    join(target_df, on=(source_df.customer_id == target_df.customer_id) & (target_df.is_current == 'Y'), how='left')\\\n",
    "    .select(source_df[\"*\"],\\\n",
    "            target_df.customer_id.alias('target_customer_id'),\\\n",
    "            target_df.name.alias('target_name'),\\\n",
    "            target_df.city.alias('target_city'),\\\n",
    "            target_df.state.alias('target_state'))\n",
    "display(join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8eb659-3aff-4cca-be9a-43b2ed676a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the join df to seperate the records which are NEW or MODIFIED\n",
    "from pyspark.sql.functions import xxhash64\n",
    "\n",
    "filter_df = join_df.filter(xxhash64(join_df.name, join_df.city, join_df.state) != xxhash64(join_df.target_name, join_df.target_city, join_df.target_state))\n",
    "\n",
    "display(filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2506e8b8-49a9-4a92-8540-95a0439bf0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new MERGED_KEY on the above df\n",
    "from pyspark.sql.functions import concat\n",
    "\n",
    "# if our table has more PK's then they should be consider into the concat\n",
    "merged_df = filter_df.withColumn('MERGEDKEY', concat(filter_df.customer_id))\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750ec4f5-66c2-455f-8d1e-b750dfe594b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new df with null MERGED_KEY on the MODIFIED rows\n",
    "dummy_df = filter_df.filter(\"target_customer_id is not null\").withColumn('MERGEDKEY', lit(None))\n",
    "display(dummy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6004791-62aa-41fe-aa2b-a5101e564042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union Between the above two df's\n",
    "# We have produced a df which has one record for the new INSERT rows and two records for the MODIFIED rows\n",
    "union_df = merged_df.union(dummy_df)\n",
    "display(union_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721a4519-53f3-4dff-8d03-88aa66d60a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_df.merge(\n",
    "    source = union_df,\n",
    "    condition = \"target_df.customer_id == union_df.customer_id and target_df.is_current == 'Y'\", # it filters the active records\n",
    ").whenMatchedUpdate(set=\n",
    "    {\n",
    "        \"is_current\" : \"'N'\",\n",
    "        \"end_date\" : \"current_date\"\n",
    "    }\n",
    ").whenNotMatchedInsert(values=\n",
    "    {\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"city\": \"source.city\",\n",
    "        \"state\": \"source.state\",\n",
    "        \"start_date\": \"current_date\",\n",
    "        \"end_date\": \"null\",\n",
    "        \"is_current\": \"'Y'\"\n",
    "    }\n",
    ").execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD Type2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
