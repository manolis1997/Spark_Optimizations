{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818a7054-4511-4019-b854-046f6199d39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"LargerDataFrames\").getOrCreate()\n",
    "\n",
    "# Employee Data (emp)\n",
    "emp_data = [\n",
    "    (1, \"Alice\", 101, 60000),\n",
    "    (2, \"Bob\", 102, 70000),\n",
    "    (3, \"Charlie\", 103, 50000),\n",
    "    (4, \"David\", 101, 45000),\n",
    "    (5, \"Eva\", 104, 75000),\n",
    "    (6, \"Frank\", 102, 72000),\n",
    "    (7, \"Grace\", 103, 48000),\n",
    "    (8, \"Hank\", 105, 80000),\n",
    "    (9, \"Ivy\", 106, 67000),\n",
    "    (10, \"Jack\", 101, 51000),\n",
    "    (11, \"Karen\", 102, 62000),\n",
    "    (12, \"Leo\", 104, 59000),\n",
    "    (13, \"Mona\", 105, 85000),\n",
    "    (14, \"Nate\", 106, 64000),\n",
    "    (15, \"Olivia\", 101, 56000)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "# Department Data (dept)\n",
    "dept_data = [\n",
    "    (101, \"HR\"),\n",
    "    (102, \"Engineering\"),\n",
    "    (103, \"Marketing\"),\n",
    "    (104, \"Finance\"),\n",
    "    (105, \"Sales\"),\n",
    "    (106, \"IT\")\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"dept_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918cb80f-49aa-4d4b-8b82-fc06935560b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_df = emp.join(dept, emp.dept_id == dept.dept_id).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8634363f-d179-4426-afd0-b997d5369152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1dc51e-b68d-40c4-8729-7f0fd387bb76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d481fabb-bb58-4211-bcc2-811d07ec33b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View for each record the partition id is into\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp_new = emp.withColumn(\"partition_id\", spark_partition_id())\n",
    "emp_new.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67419de1-af4f-4f61-bbcf-deea1a735143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The same dept_id is into the same partition\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp_new = emp.repartition(4, 'dept_id').withColumn(\"partition_id\", spark_partition_id())\n",
    "emp_new.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1c7b8e-c1b0-4bff-88cf-c7f969331b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The same dept_id is into the same partition\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "dept_new = dept.repartition(4, 'dept_id').withColumn(\"partition_id\", spark_partition_id())\n",
    "dept_new.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8308d2-60aa-40ee-9cf0-f8736484f47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_df_new = emp_new.join(dept_new, emp_new.dept_id == dept_new.dept_id).explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Repartition-Coalesce",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
